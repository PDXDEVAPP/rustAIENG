[package]
name = "rust_ollama"
version = "0.2.0"
edition = "2021"
authors = ["MiniMax Agent"]
description = "A high-performance, modular LLM inference server with Ollama-compatible API"
license = "MIT"

[dependencies]
# Web framework
axum = { version = "0.7", features = ["json", "ws", "multipart"] }
tower = { version = "0.4", features = ["util", "timeout", "limit", "load-shed"] }
tower-http = { version = "0.5", features = ["cors", "trace", "compression-br"] }
hyper = { version = "1.0", features = ["full"] }
tokio = { version = "1.0", features = ["full"] }
warp = "0.3"

# HTTP client/server utilities
reqwest = { version = "0.11", features = ["json", "stream"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
bytes = "1.0"
http = "1.0"

# LLM Inference (Enhanced)
candle-core = "0.6"
candle-nn = "0.6" 
candle-transformers = "0.6"
tokenizers = "0.19"
# Add HuggingFace Hub support for model downloads
hf-hub = { version = "0.3", features = ["tokio", "async"] }

# Database
sqlx = { version = "0.7", features = ["runtime-tokio-rustls", "sqlite", "chrono", "json"] }

# Real-time communication
tungstenite = { version = "0.21", features = ["native-tls"], optional = true }
websocket = "0.3"

# Performance monitoring
prometheus = "0.13"
metrics = "0.22"
metrics-exporter-prometheus = "0.12"
opentelemetry = { version = "0.18", features = ["metrics", "trace"], optional = true }
opentelemetry-otlp = { version = "0.11", optional = true }
opentelemetry_sdk = { version = "0.18", optional = true }
opentelemetry-semantic-conventions = { version = "0.11", optional = true }
opentelemetry-stdout = { version = "0.1", optional = true }
tracing-opentelemetry = { version = "0.18", optional = true }

# Utilities
anyhow = "1.0"
thiserror = "1.0"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["fmt", "chrono", "env-filter"] }
clap = { version = "4.0", features = ["derive", "cargo"] }
config = "0.14"
once_cell = "1.0"
uuid = { version = "1.0", features = ["v4"] }
chrono = { version = "0.4", features = ["serde"] }
tokio-util = { version = "0.7", features = ["codec"] }
futures = "0.3"
async-trait = "0.1"
dashmap = "5.4"
parking_lot = "0.12"

# Logging
env_logger = "0.10"
log = "0.4"

# CLI enhancements
crossterm = { version = "0.27", features = ["event-stream"] }
tui = { version = "0.19", features = ["crossterm"] }
serde_yaml = "0.9"

# Vector embeddings and search
annoy = { version = "0.1", optional = true }

# Machine learning utilities
ndarray = "0.15"

# Security & Authentication
jsonwebtoken = "9.0"
bcrypt = "0.15"
hmac = "0.12"
sha2 = "0.10"
ring = "0.17"
md5 = "0.8"

# File monitoring and hot reload
notify = "6.0"
walkdir = "2.4"

# Kubernetes integration (optional - production only)
kube = { version = "0.87", features = ["runtime", "derive"], optional = true }

# Message queues and async processing
redis = { version = "0.24", features = ["tokio-comp"], optional = true }
amqprs = { version = "2.1", optional = true }
lapin = { version = "2.3", optional = true }

# Cloud storage providers
aws-sdk-s3 = { version = "1.0", optional = true }
aws-config = { version = "1.0", optional = true }
azure_storage_blobs = { version = "0.17", optional = true }
azure_identity = { version = "0.17", optional = true }
azure_core = { version = "0.17", optional = true }

# Compression and caching
flate2 = "1.0"
lz4_flex = "0.11"
zstd = "0.13"
lru = "0.12"

# Image processing and multimodal
image = "0.25"

# Model ensemble and quantization
rand = "0.8"

# Model storage and registry
toml = "0.8"

[build-dependencies]
cc = "1.0"

[dev-dependencies]
k8s-openapi = { version = "0.21", features = ["v1_30"] }

[features]
default = ["candle-core", "candle-nn", "candle-transformers", "tokenizers", "websocket-support", "advanced-monitoring"]
candle-cuda = ["candle-core/cuda", "candle-nn/cuda"]
metal = ["candle-core/metal"]
websocket-support = ["tungstenite"]
advanced-monitoring = ["opentelemetry", "opentelemetry-otlp"]
vector-search = ["annoy"]
kubernetes = ["kube"]

[[bin]]
name = "rust_ollama"
path = "src/main.rs"

[[bin]]
name = "ollama_cli"
path = "src/bin/ollama_cli.rs"

[[bin]]
name = "ollama_tui"
path = "src/bin/ollama_tui.rs"

[[bin]]
name = "model_finetuner"
path = "src/bin/model_finetuner.rs"

[[bin]]
name = "stress_test"
path = "src/bin/stress_test.rs"