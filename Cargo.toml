[package]
name = "rust_ollama"
version = "0.2.0"
edition = "2021"
authors = ["MiniMax Agent"]
description = "A high-performance, modular LLM inference server with Ollama-compatible API"
license = "MIT"

[dependencies]
# Web framework
axum = { version = "0.7", features = ["json", "ws", "multipart"] }
tower = { version = "0.4", features = ["util", "timeout", "limit", "load-shed"] }
tower-http = { version = "0.5", features = ["cors", "trace", "compression-br"] }
hyper = { version = "1.0", features = ["full"] }
tokio = { version = "1.0", features = ["full"] }
warp = "0.3"

# HTTP client/server utilities
reqwest = { version = "0.11", features = ["json", "stream"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
bytes = "1.0"
http = "1.0"

# LLM Inference (Updated for rand compatibility)
candle-core = "0.9"
candle-nn = "0.9" 
candle-transformers = "0.9"
tokenizers = "0.19"
# HuggingFace Hub support for model downloads
hf-hub = { version = "0.3", features = ["tokio"] }

# Database
sqlx = { version = "0.7", features = ["runtime-tokio-rustls", "sqlite", "chrono", "json"] }

# Real-time communication
tungstenite = { version = "0.21", features = ["native-tls"], optional = true }

# Performance monitoring (simplified)
prometheus = "0.13"
metrics = "0.22"
metrics-exporter-prometheus = "0.12"

# Utilities
anyhow = "1.0"
thiserror = "1.0"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["fmt", "chrono", "env-filter"] }
clap = { version = "4.0", features = ["derive", "cargo"] }
config = "0.14"
once_cell = "1.0"
uuid = { version = "1.0", features = ["v4"] }
chrono = { version = "0.4", features = ["serde"] }
tokio-util = { version = "0.7", features = ["codec"] }
futures = "0.3"
async-trait = "0.1"
dashmap = "5.4"
parking_lot = "0.12"

# Logging
env_logger = "0.10"
log = "0.4"

# CLI enhancements
crossterm = { version = "0.27", features = ["event-stream"] }
tui = { version = "0.19", features = ["crossterm"] }
serde_yaml = "0.9"

# Machine learning utilities
ndarray = "0.15"
rand = "0.9"

# Security & Authentication
jsonwebtoken = "9.0"
bcrypt = "0.15"
hmac = "0.12"
sha2 = "0.10"
ring = "0.17"
md5 = "0.8"

# File monitoring and hot reload
notify = "6.0"
walkdir = "2.4"

# Message queues and async processing
redis = { version = "0.24", features = ["tokio-comp"], optional = true }

# Cloud storage providers
aws-sdk-s3 = { version = "1.0", optional = true }
aws-config = { version = "1.0", optional = true }

# Compression and caching
flate2 = "1.0"
lz4_flex = "0.11"
zstd = "0.13"
lru = "0.12"

# Image processing and multimodal
image = "0.25"

# Model ensemble and quantization
# rand removed from here - it's already declared above

# Model storage and registry
toml = "0.8"

[build-dependencies]
cc = "1.0"

[features]
# Minimal feature set to avoid conflicts
default = ["websocket-support"]
websocket-support = ["tungstenite"]

[[bin]]
name = "rust_ollama"
path = "src/main.rs"

[[bin]]
name = "ollama_cli"
path = "src/bin/ollama_cli.rs"

[[bin]]
name = "ollama_tui"
path = "src/bin/ollama_tui.rs"

[[bin]]
name = "model_finetuner"
path = "src/bin/model_finetuner.rs"

[[bin]]
name = "stress_test"
path = "src/bin/stress_test.rs"